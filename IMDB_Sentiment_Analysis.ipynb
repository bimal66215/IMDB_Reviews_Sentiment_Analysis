{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "EIg_epKMkFFL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from keras import Sequential\n",
        "from keras.layers import Dense,Embedding,Flatten, LSTM,BatchNormalization\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "# !pip install emoji\n",
        "import emoji\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "# nltk.download('stopwords')\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "StopWords = stopwords.words('english')\n",
        "from sklearn.utils import class_weight\n",
        "import os\n",
        "from keras.models import load_model\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        " device_name = os.environ['COLAB_TPU_ADDR']\n",
        " TPU_ADDRESS = 'grpc://' + device_name\n",
        " print('Found TPU at: {}'.format(TPU_ADDRESS))\n",
        "except KeyError:\n",
        " print('TPU not found')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNZR8ukeF_Ve",
        "outputId": "bf84a437-d02c-4370-8103-3f5b38dd10de"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found TPU at: grpc://10.8.250.90:8470\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('IMDB Dataset.csv',error_bad_lines=False, engine=\"python\")"
      ],
      "metadata": {
        "id": "joe_MVR-nzKi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8de4d846-41b8-403e-b669-64c1705cf31f"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-68-80cfbe6f977a>:1: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
            "\n",
            "\n",
            "  df = pd.read_csv('IMDB Dataset.csv',error_bad_lines=False, engine=\"python\")\n",
            "Skipping line 34833: unexpected end of data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df['review'], df['sentiment'],test_size=0.25)"
      ],
      "metadata": {
        "id": "QL6gHTiaoB1M"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Preprocessing\n"
      ],
      "metadata": {
        "id": "2YkYI2d8P29M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def text_preprocess(text_):\n",
        "  out = []\n",
        "  text_ = text_.lower()  ## lower case\n",
        "\n",
        "  pattern = re.compile(r'https?://\\S+|www\\.\\S+') ## remove URLs\n",
        "  text_ = pattern.sub(r'', text_)\n",
        "\n",
        "  text_ = BeautifulSoup(text_).get_text() ## remove html tags\n",
        "\n",
        "  text_ = emoji.demojize(text_)  ##convert emojis to text\n",
        "\n",
        "  new_text = []                  ## remove stop words\n",
        "  for word in text_.split():\n",
        "      if word in StopWords:\n",
        "          new_text.append('')\n",
        "      else:\n",
        "          new_text.append(word)\n",
        "  text_= \" \".join(new_text)\n",
        "\n",
        "  doc = nlp(text_)         ## Lemmatization\n",
        "  text_= \" \".join([token.lemma_ for token in doc])\n",
        "\n",
        "  return re.sub(' +', ' ', text_)\n",
        "\n",
        "X_train =  X_train.apply(text_preprocess).to_numpy()\n",
        "y_train = y_train.apply(text_preprocess).to_numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNJFN-SaoSGU",
        "outputId": "df1fefb8-be32-4cc2-c9ef-bae68fae374f"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-71-26cefacf822f>:8: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  text_ = BeautifulSoup(text_).get_text() ## remove html tags\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size=15000\n",
        "output_dim = 20\n",
        "max_length = 700\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "learning_rate = 0.001\n",
        "\n",
        "## tokenization\n",
        "tokenizer = Tokenizer(num_words = vocab_size, oov_token = \"<OOV>\", lower = False)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_train_clean = np.array(tokenizer.texts_to_sequences(X_train))\n",
        "X_test_clean = np.array(tokenizer.texts_to_sequences(X_test))\n",
        "\n",
        "\n",
        "## padding\n",
        "X_train_clean = pad_sequences(X_train_clean, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "X_test_clean = pad_sequences(X_test_clean, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "\n",
        "## Tokenize and remove special characters\n",
        "map_dict = {'positive':1,\n",
        "            'negative':0}\n",
        "\n",
        "y_train_coded = np.array(list(map(lambda x: map_dict[x],y_train)))\n",
        "y_test_coded = np.array(list(map(lambda x: map_dict[x],y_test)))\n",
        "\n",
        "## Class Wieght\n",
        "# class_weights = class_weight.compute_class_weight(\n",
        "#                                         class_weight = \"balanced\",\n",
        "#                                         classes = np.unique(y_train_coded),\n",
        "#                                         y = y_train_coded\n",
        "#                                     )\n",
        "# class_weights = dict(zip(np.unique(y_train_coded), class_weights))\n",
        "\n",
        "tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "tf.config.run_functions_eagerly(False)\n",
        "tf.config.experimental_connect_to_cluster(tpu)\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "strategy = tf.distribute.TPUStrategy(tpu)\n",
        "\n",
        "with strategy.scope():\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim = vocab_size,output_dim=output_dim,input_length=max_length))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LSTM(128,return_sequences=True, dropout=0.2, recurrent_dropout=0.2))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LSTM(128,return_sequences=False, dropout=0.2, recurrent_dropout=0.2))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39RbnixibCbF",
        "outputId": "03260cbd-9329-4b35-a937-705130bfeca3"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-73-7a075ffef155>:12: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  X_train_clean = np.array(tokenizer.texts_to_sequences(X_train))\n",
            "<ipython-input-73-7a075ffef155>:13: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  X_test_clean = np.array(tokenizer.texts_to_sequences(X_test))\n",
            "WARNING:tensorflow:TPU system grpc://10.8.250.90:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n",
            "WARNING:tensorflow:TPU system grpc://10.8.250.90:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_12 (Embedding)    (None, 700, 20)           300000    \n",
            "                                                                 \n",
            " batch_normalization_36 (Bat  (None, 700, 20)          80        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " lstm_24 (LSTM)              (None, 700, 128)          76288     \n",
            "                                                                 \n",
            " batch_normalization_37 (Bat  (None, 700, 128)         512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " lstm_25 (LSTM)              (None, 128)               131584    \n",
            "                                                                 \n",
            " batch_normalization_38 (Bat  (None, 128)              512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 509,105\n",
            "Trainable params: 508,553\n",
            "Non-trainable params: 552\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train_clean, y_train_coded,epochs=50,validation_data=(X_test_clean,y_test_coded),  verbose = 2) ##class_weight = class_weights,\n",
        "name = 'IMDB_sentiment_Analysis_{}.h5'.format(datetime.datetime.now().strftime('%dth_%B_%H_%M_%S'))\n",
        "model.save(name)  # creates a HDF5 file 'my_model.h5'\n",
        "# del model  # deletes the existing model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDYcTS1uIWAY",
        "outputId": "ae0e7f20-0ffd-4b72-b5f3-d507695fb311"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "817/817 - 182s - loss: 0.7200 - acc: 0.5009 - val_loss: 0.8774 - val_acc: 0.4990 - 182s/epoch - 222ms/step\n",
            "Epoch 2/50\n",
            "817/817 - 151s - loss: 0.6967 - acc: 0.4983 - val_loss: 0.7070 - val_acc: 0.5006 - 151s/epoch - 185ms/step\n",
            "Epoch 3/50\n",
            "817/817 - 151s - loss: 0.6973 - acc: 0.5019 - val_loss: 0.7846 - val_acc: 0.4997 - 151s/epoch - 185ms/step\n",
            "Epoch 4/50\n",
            "817/817 - 151s - loss: 0.6944 - acc: 0.4998 - val_loss: 0.7002 - val_acc: 0.4974 - 151s/epoch - 185ms/step\n",
            "Epoch 5/50\n",
            "817/817 - 151s - loss: 0.6943 - acc: 0.4962 - val_loss: 0.7273 - val_acc: 0.4994 - 151s/epoch - 184ms/step\n",
            "Epoch 6/50\n",
            "817/817 - 151s - loss: 0.6941 - acc: 0.4972 - val_loss: 0.7623 - val_acc: 0.5001 - 151s/epoch - 185ms/step\n",
            "Epoch 7/50\n",
            "817/817 - 151s - loss: 0.6940 - acc: 0.4939 - val_loss: 0.7020 - val_acc: 0.5034 - 151s/epoch - 185ms/step\n",
            "Epoch 8/50\n",
            "817/817 - 151s - loss: 0.6937 - acc: 0.4986 - val_loss: 1.0242 - val_acc: 0.4954 - 151s/epoch - 184ms/step\n",
            "Epoch 9/50\n",
            "817/817 - 151s - loss: 0.6939 - acc: 0.4974 - val_loss: 0.8037 - val_acc: 0.5039 - 151s/epoch - 184ms/step\n",
            "Epoch 10/50\n",
            "817/817 - 151s - loss: 0.6936 - acc: 0.5037 - val_loss: 0.9180 - val_acc: 0.5073 - 151s/epoch - 185ms/step\n",
            "Epoch 11/50\n",
            "817/817 - 151s - loss: 0.6937 - acc: 0.4986 - val_loss: 0.7099 - val_acc: 0.5008 - 151s/epoch - 184ms/step\n",
            "Epoch 12/50\n",
            "817/817 - 151s - loss: 0.6936 - acc: 0.4964 - val_loss: 0.9275 - val_acc: 0.5002 - 151s/epoch - 185ms/step\n",
            "Epoch 13/50\n",
            "817/817 - 152s - loss: 0.6934 - acc: 0.4989 - val_loss: 0.9168 - val_acc: 0.5002 - 152s/epoch - 186ms/step\n",
            "Epoch 14/50\n",
            "817/817 - 151s - loss: 0.6935 - acc: 0.4971 - val_loss: 0.8964 - val_acc: 0.5044 - 151s/epoch - 185ms/step\n",
            "Epoch 15/50\n",
            "817/817 - 151s - loss: 0.6932 - acc: 0.5058 - val_loss: 0.9621 - val_acc: 0.4958 - 151s/epoch - 185ms/step\n",
            "Epoch 16/50\n",
            "817/817 - 151s - loss: 0.6934 - acc: 0.4995 - val_loss: 0.8627 - val_acc: 0.5008 - 151s/epoch - 185ms/step\n",
            "Epoch 17/50\n",
            "817/817 - 151s - loss: 0.6941 - acc: 0.4996 - val_loss: 0.7399 - val_acc: 0.5045 - 151s/epoch - 185ms/step\n",
            "Epoch 18/50\n",
            "817/817 - 151s - loss: 0.6935 - acc: 0.5045 - val_loss: 0.7592 - val_acc: 0.5055 - 151s/epoch - 184ms/step\n",
            "Epoch 19/50\n",
            "817/817 - 151s - loss: 0.6936 - acc: 0.4971 - val_loss: 0.7450 - val_acc: 0.5001 - 151s/epoch - 184ms/step\n",
            "Epoch 20/50\n",
            "817/817 - 151s - loss: 0.6936 - acc: 0.4980 - val_loss: 0.8563 - val_acc: 0.5015 - 151s/epoch - 185ms/step\n",
            "Epoch 21/50\n",
            "817/817 - 151s - loss: 0.6934 - acc: 0.5020 - val_loss: 0.7366 - val_acc: 0.5039 - 151s/epoch - 184ms/step\n",
            "Epoch 22/50\n",
            "817/817 - 151s - loss: 0.6936 - acc: 0.4992 - val_loss: 0.7327 - val_acc: 0.5042 - 151s/epoch - 185ms/step\n",
            "Epoch 23/50\n",
            "817/817 - 151s - loss: 0.6935 - acc: 0.4997 - val_loss: 0.9839 - val_acc: 0.5008 - 151s/epoch - 184ms/step\n",
            "Epoch 24/50\n",
            "817/817 - 151s - loss: 0.6933 - acc: 0.5036 - val_loss: 0.9860 - val_acc: 0.5041 - 151s/epoch - 184ms/step\n",
            "Epoch 25/50\n",
            "817/817 - 151s - loss: 0.6934 - acc: 0.5015 - val_loss: 1.0320 - val_acc: 0.5047 - 151s/epoch - 185ms/step\n",
            "Epoch 26/50\n",
            "817/817 - 151s - loss: 0.6934 - acc: 0.4987 - val_loss: 0.9276 - val_acc: 0.5063 - 151s/epoch - 184ms/step\n",
            "Epoch 27/50\n",
            "817/817 - 151s - loss: 0.6932 - acc: 0.4992 - val_loss: 1.0949 - val_acc: 0.5000 - 151s/epoch - 185ms/step\n",
            "Epoch 28/50\n",
            "817/817 - 151s - loss: 0.6932 - acc: 0.5061 - val_loss: 1.0564 - val_acc: 0.4986 - 151s/epoch - 185ms/step\n",
            "Epoch 29/50\n",
            "817/817 - 151s - loss: 0.6933 - acc: 0.5024 - val_loss: 1.2566 - val_acc: 0.5003 - 151s/epoch - 184ms/step\n",
            "Epoch 30/50\n",
            "817/817 - 151s - loss: 0.6936 - acc: 0.4909 - val_loss: 0.9206 - val_acc: 0.5037 - 151s/epoch - 184ms/step\n",
            "Epoch 31/50\n",
            "817/817 - 151s - loss: 0.6932 - acc: 0.5013 - val_loss: 1.0819 - val_acc: 0.5040 - 151s/epoch - 184ms/step\n",
            "Epoch 32/50\n",
            "817/817 - 150s - loss: 0.6301 - acc: 0.6204 - val_loss: 1.1294 - val_acc: 0.7282 - 150s/epoch - 184ms/step\n",
            "Epoch 33/50\n",
            "817/817 - 151s - loss: 0.4598 - acc: 0.8178 - val_loss: 1.0090 - val_acc: 0.7424 - 151s/epoch - 185ms/step\n",
            "Epoch 34/50\n",
            "817/817 - 151s - loss: 0.3874 - acc: 0.8573 - val_loss: 0.9096 - val_acc: 0.7959 - 151s/epoch - 185ms/step\n",
            "Epoch 35/50\n",
            "817/817 - 151s - loss: 0.3621 - acc: 0.8613 - val_loss: 0.7769 - val_acc: 0.8042 - 151s/epoch - 185ms/step\n",
            "Epoch 36/50\n",
            "817/817 - 151s - loss: 0.3406 - acc: 0.8685 - val_loss: 0.9380 - val_acc: 0.7822 - 151s/epoch - 185ms/step\n",
            "Epoch 37/50\n",
            "817/817 - 151s - loss: 0.3088 - acc: 0.8845 - val_loss: 0.8088 - val_acc: 0.7833 - 151s/epoch - 184ms/step\n",
            "Epoch 38/50\n",
            "817/817 - 151s - loss: 0.2988 - acc: 0.8889 - val_loss: 1.1159 - val_acc: 0.8020 - 151s/epoch - 184ms/step\n",
            "Epoch 39/50\n",
            "817/817 - 151s - loss: 0.2833 - acc: 0.8968 - val_loss: 0.8620 - val_acc: 0.8268 - 151s/epoch - 185ms/step\n",
            "Epoch 40/50\n",
            "817/817 - 151s - loss: 0.2654 - acc: 0.9012 - val_loss: 1.0063 - val_acc: 0.8075 - 151s/epoch - 185ms/step\n",
            "Epoch 41/50\n",
            "817/817 - 151s - loss: 0.2638 - acc: 0.8986 - val_loss: 1.0522 - val_acc: 0.8027 - 151s/epoch - 184ms/step\n",
            "Epoch 42/50\n",
            "817/817 - 151s - loss: 0.2583 - acc: 0.9040 - val_loss: 1.0987 - val_acc: 0.7916 - 151s/epoch - 185ms/step\n",
            "Epoch 43/50\n",
            "817/817 - 151s - loss: 0.2587 - acc: 0.9075 - val_loss: 1.3198 - val_acc: 0.8137 - 151s/epoch - 185ms/step\n",
            "Epoch 44/50\n",
            "817/817 - 151s - loss: 0.2331 - acc: 0.9117 - val_loss: 1.0982 - val_acc: 0.8242 - 151s/epoch - 185ms/step\n",
            "Epoch 45/50\n",
            "817/817 - 151s - loss: 0.2301 - acc: 0.9121 - val_loss: 0.9109 - val_acc: 0.8120 - 151s/epoch - 185ms/step\n",
            "Epoch 46/50\n",
            "817/817 - 150s - loss: 0.2365 - acc: 0.9107 - val_loss: 0.8744 - val_acc: 0.8311 - 150s/epoch - 184ms/step\n",
            "Epoch 47/50\n",
            "817/817 - 151s - loss: 0.3158 - acc: 0.8852 - val_loss: 1.5647 - val_acc: 0.8109 - 151s/epoch - 184ms/step\n",
            "Epoch 48/50\n",
            "817/817 - 151s - loss: 0.2449 - acc: 0.9063 - val_loss: 1.4861 - val_acc: 0.8132 - 151s/epoch - 185ms/step\n",
            "Epoch 49/50\n",
            "817/817 - 151s - loss: 0.2235 - acc: 0.9143 - val_loss: 1.2999 - val_acc: 0.8206 - 151s/epoch - 184ms/step\n",
            "Epoch 50/50\n",
            "817/817 - 152s - loss: 0.2043 - acc: 0.9170 - val_loss: 1.3879 - val_acc: 0.8228 - 152s/epoch - 186ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model.predict('Movie was really great!')\n",
        "def model_test(input_text):\n",
        "  if type(input_text)==\n",
        "  input_text = text_preprocess(input_text)\n",
        "  input_text_clean = np.array(tokenizer.texts_to_sequences([input_text]))\n",
        "\n",
        "  ## padding\n",
        "  input_text_padded = pad_sequences(input_text_clean, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "  return input_text_padded\n",
        "\n",
        "model.predict(model_test('Movie was really good but ac was too cold and irritating!'))\n",
        "# returns a compiled model\n",
        "# identical to the previous one\n",
        "# model = load_model('my_model.h5')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_AiAJPaWxC3",
        "outputId": "99e01f10-4a20-4c1f-acdc-08e907749347"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 692ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.0076232]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    }
  ]
}